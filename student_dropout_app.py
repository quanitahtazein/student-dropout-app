# -*- coding: utf-8 -*-
"""Student_dropout_app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_tD_kPMroyyZxddA2cN1agOIHssB-7N6
"""

import pandas as pd

df = pd.read_csv('/content/dropout set.zip')

data = df.copy()

data.info()

data.describe()

"""**Set Target:**"""

#treat L as a risk for dropout
df["dropout"] = df["Class"].map({"L": 1, "M": 0, "H": 0})

"""**Check For Null Values**"""

#check for null values
df.isnull().values.any()

"""**Drop unecessary columns**"""

cols_to_drop = [
    "Nationality", "PlaceofBirth", "Topic", "Semester", "SectionID",
    "Relation", "Class"]  # drop after creating target

# Only drop if exists
df = df.drop(columns=[c for c in cols_to_drop if c in df.columns])

"""**Define Functions**"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

num_features = [
    "raisedhands", "VisITedResources",
    "AnnouncementsView", "Discussion"]

cat_features = [
    "gender", "StageID", "GradeID",
    "ParentAnsweringSurvey",
    "ParentschoolSatisfaction",
    "StudentAbsenceDays"]

numeric_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())])

categorical_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OneHotEncoder(handle_unknown="ignore"))])

preprocessor = ColumnTransformer([("num", numeric_pipeline, num_features),("cat", categorical_pipeline, cat_features)])

"""**Train and test set**"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y,
    test_size=0.2,       # 20% of data for testing
    stratify=y,          # keep same dropout ratio in train & test
    random_state=42)

print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)

from sklearn.linear_model import LogisticRegression

logreg_pipeline = Pipeline([
    ("preprocessor", preprocessor),
    ("model", LogisticRegression(max_iter=1000, class_weight="balanced"))])

logreg_pipeline.fit(X_train, y_train)

from sklearn.metrics import classification_report, roc_auc_score

y_pred = logreg_pipeline.predict(X_test)
y_prob = logreg_pipeline.predict_proba(X_test)[:,1]  # risk scores

print(classification_report(y_test, y_pred))
print("ROC AUC:", roc_auc_score(y_test, y_prob))

def risk_label(score):
    if score >= 0.7:
        return "High"
    elif score >= 0.4:
        return "Medium"
    else:
        return "Low"

predictions = X_test.copy()
predictions["risk_score"] = y_prob
predictions["risk_label"] = [risk_label(s) for s in y_prob]
predictions["predicted_dropout"] = (y_prob >= 0.5).astype(int)

# Save predictions CSV
predictions.to_csv("predictions.csv", index=False)

print("âœ… Logistic Regression model trained and predictions saved!")

from sklearn.tree import DecisionTreeClassifier

# Use the same numeric and categorical features you defined earlier
# and the same preprocessing pipeline (preprocessor)

dt_pipeline = Pipeline([
    ("preprocessor", preprocessor),
    ("model", DecisionTreeClassifier(max_depth=5, class_weight="balanced", random_state=42))])

dt_pipeline.fit(X_train, y_train)

y_pred_dt = dt_pipeline.predict(X_test)
y_prob_dt = dt_pipeline.predict_proba(X_test)[:, 1]  # probability of dropout

from sklearn.metrics import classification_report, roc_auc_score

print("=== Decision Tree Evaluation ===")
print(classification_report(y_test, y_pred_dt))
print("ROC AUC:", roc_auc_score(y_test, y_prob_dt))

def risk_label(score):
    if score >= 0.7:
        return "High"
    elif score >= 0.4:
        return "Medium"
    else:
        return "Low"

predictions_dt = X_test.copy()
predictions_dt["risk_score"] = y_prob_dt
predictions_dt["risk_label"] = [risk_label(s) for s in y_prob_dt]
predictions_dt["predicted_dropout"] = (y_prob_dt >= 0.5).astype(int)

# Save predictions CSV
predictions_dt.to_csv("predictions_dt.csv", index=False)

print("âœ… Decision Tree predictions saved!")

from sklearn.ensemble import RandomForestClassifier

# Reuse the same preprocessing pipeline (preprocessor)
rf_pipeline = Pipeline([
    ("preprocessor", preprocessor),
    ("model", RandomForestClassifier(n_estimators=100,max_depth=7,class_weight="balanced",random_state=42))])

rf_pipeline.fit(X_train, y_train)

y_pred_rf = rf_pipeline.predict(X_test)
y_prob_rf = rf_pipeline.predict_proba(X_test)[:, 1]  # probability of dropout

from sklearn.metrics import classification_report, roc_auc_score

print("=== Random Forest Evaluation ===")
print(classification_report(y_test, y_pred_rf))
print("ROC AUC:", roc_auc_score(y_test, y_prob_rf))

def risk_label(score):
    if score >= 0.7:
        return "High"
    elif score >= 0.4:
        return "Medium"
    else:
        return "Low"

predictions_rf = X_test.copy()
predictions_rf["risk_score"] = y_prob_rf
predictions_rf["risk_label"] = [risk_label(s) for s in y_prob_rf]
predictions_rf["predicted_dropout"] = (y_prob_rf >= 0.5).astype(int)

# Save predictions CSV
predictions_rf.to_csv("predictions_rf.csv", index=False)

print("âœ… Random Forest predictions saved!")

# y_train is your target in train set
num_negative = (y_train == 0).sum()
num_positive = (y_train == 1).sum()

class_weight_ratio = num_negative / num_positive
print("scale_pos_weight =", class_weight_ratio)

from xgboost import XGBClassifier
from sklearn.pipeline import Pipeline

xgb_pipeline = Pipeline([
    ("preprocessor", preprocessor),
    ("model", XGBClassifier(n_estimators=100,max_depth=5,learning_rate=0.1,
        scale_pos_weight=class_weight_ratio,  # handles imbalance
        use_label_encoder=False,  # avoids warning in recent xgboost
        eval_metric='logloss',    # needed with use_label_encoder=False
        random_state=42))])

xgb_pipeline.fit(X_train, y_train)

y_pred_xgb = xgb_pipeline.predict(X_test)
y_prob_xgb = xgb_pipeline.predict_proba(X_test)[:,1]

from sklearn.metrics import classification_report, roc_auc_score

print("=== XGBoost Evaluation ===")
print(classification_report(y_test, y_pred_xgb))
print("ROC AUC:", roc_auc_score(y_test, y_prob_xgb))

def risk_label(score):
    if score >= 0.7:
        return "High"
    elif score >= 0.4:
        return "Medium"
    else:
        return "Low"

predictions_xgb = X_test.copy()
predictions_xgb["risk_score"] = y_prob_xgb
predictions_xgb["risk_label"] = [risk_label(s) for s in y_prob_xgb]
predictions_xgb["predicted_dropout"] = (y_prob_xgb >= 0.5).astype(int)

predictions_xgb.to_csv("predictions_xgb.csv", index=False)
print("âœ… XGBoost predictions saved!")

from sklearn.metrics import roc_auc_score

# y_test: true labels
# y_prob_*: predicted probabilities for positive class (dropout)

roc_scores = {
    "Logistic Regression": roc_auc_score(y_test, y_prob),
    "Decision Tree": roc_auc_score(y_test, y_prob_dt),
    "Random Forest": roc_auc_score(y_test, y_prob_rf),
    "XGBoost": roc_auc_score(y_test, y_prob_xgb)}

print("ROC AUC scores:", roc_scores)

import matplotlib.pyplot as plt
import seaborn as sns

# Convert to list
models = list(roc_scores.keys())
scores = list(roc_scores.values())

plt.figure(figsize=(8,5))
sns.barplot(x=models, y=scores, palette="viridis")
plt.ylim(0, 1)
plt.ylabel("ROC AUC")
plt.title("Comparison of Models on Dropout Prediction")
plt.xticks(rotation=15)
for i, v in enumerate(scores):
    plt.text(i, v + 0.02, f"{v:.2f}", ha='center', fontweight='bold')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Suppose your trained model is rf_pipeline (Random Forest)
# Get feature names after preprocessing
preprocessor = rf_pipeline.named_steps['preprocessor']

# Get numeric feature names
num_features = preprocessor.transformers_[0][2]  # numeric columns

# Get categorical feature names after one-hot encoding
cat_pipeline = preprocessor.transformers_[1][1]
cat_features = preprocessor.transformers_[1][2]
ohe = cat_pipeline.named_steps['encoder']
cat_feature_names = ohe.get_feature_names_out(cat_features)

# Combine all feature names
all_features = list(num_features) + list(cat_feature_names)

# Get feature importances
importances = rf_pipeline.named_steps['model'].feature_importances_

# Create DataFrame
feat_imp = pd.DataFrame({"feature": all_features, "importance": importances})
feat_imp = feat_imp.sort_values(by="importance", ascending=False)

# Select top 10 most important features
top_features = feat_imp.head(10)

plt.figure(figsize=(8,5))
sns.barplot(x="importance", y="feature", data=top_features, palette="magma")
plt.title("Top Reasons for Student Dropout (Feature Importance)")
plt.xlabel("Importance Score")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

pip install streamlit

uploaded_file = st.file_uploader("Upload student CSV", type=["csv"])

if uploaded_file:
    df_new = pd.read_csv(uploaded_file)
    st.write("Uploaded Data Sample:", df_new.head())

    # Load models
    logreg_model = joblib.load("dropout_model.pkl")
    dt_model = joblib.load("dropout_model_dt.pkl")
    rf_model = joblib.load("dropout_model_rf.pkl")
    xgb_model = joblib.load("dropout_model_xgb.pkl")

    # Select student index
    student_idx = st.number_input(
        "Enter student row index:", min_value=0, max_value=len(df_new)-1, value=0
    )
    student = df_new.iloc[student_idx]
    st.write(student)

    # --- Top Reasons Bar Chart ---
    st.subheader("Top Reasons for This Student's Dropout Risk")

    # Feature importance from Random Forest
    model = rf_model
    preprocessor = model.named_steps["preprocessor"]

    # Numeric features
    num_features = preprocessor.transformers_[0][2]

    # Categorical features after one-hot encoding
    cat_pipeline = preprocessor.transformers_[1][1]
    cat_features = preprocessor.transformers_[1][2]
    ohe = cat_pipeline.named_steps['encoder']
    cat_feature_names = ohe.get_feature_names_out(cat_features)

    all_features = list(num_features) + list(cat_feature_names)
    importances = model.named_steps["model"].feature_importances_
    feat_imp = pd.DataFrame({"feature": all_features, "importance": importances})
    feat_imp = feat_imp.sort_values("importance", ascending=False).head(10)

    # Plot
    fig, ax = plt.subplots(figsize=(6,4))
    sns.barplot(x="importance", y="feature", data=feat_imp, palette="magma", ax=ax)
    ax.set_xlabel("Importance")
    ax.set_title(f"Top 10 Dropout Risk Factors")
    st.pyplot(fig)

import streamlit as st
import pandas as pd
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_auc_score

st.set_page_config(page_title="Student Dropout Early Warning", layout="wide")

st.title("ðŸŽ“ Student Dropout Early Warning System")

# ---------------------------
# 1ï¸âƒ£ Upload CSV
# ---------------------------
uploaded_file = st.file_uploader("Upload student CSV", type=["csv"])

if uploaded_file:
    df_new = pd.read_csv(uploaded_file)
    st.write("Uploaded Data Sample:", df_new.head())

    # ---------------------------
    # 2ï¸âƒ£ Load trained models
    # ---------------------------
    logreg_model = joblib.load("dropout_model.pkl")      # Logistic Regression
    dt_model = joblib.load("dropout_model_dt.pkl")       # Decision Tree
    rf_model = joblib.load("dropout_model_rf.pkl")       # Random Forest
    xgb_model = joblib.load("dropout_model_xgb.pkl")     # XGBoost

    models = {
        "Logistic Regression": logreg_model,
        "Decision Tree": dt_model,
        "Random Forest": rf_model,
        "XGBoost": xgb_model
    }

    # ---------------------------
    # 3ï¸âƒ£ Prediction helper
    # ---------------------------
    def predict_risk(model, df):
        prob = model.predict_proba(df)[:,1]
        def label(score):
            if score >= 0.7:
                return "High"
            elif score >= 0.4:
                return "Medium"
            else:
                return "Low"
        risk_label = [label(s) for s in prob]
        return prob, risk_label

    # ---------------------------
    # 4ï¸âƒ£ Predict using Logistic Regression (default)
    # ---------------------------
    risk_scores, risk_labels = predict_risk(logreg_model, df_new)
    df_new["risk_score"] = risk_scores
    df_new["risk_label"] = risk_labels

    # ---------------------------
    # 5ï¸âƒ£ Show Top 20 high-risk students
    # ---------------------------
    st.subheader("Top 20 High-Risk Students")
    top_risk = df_new.sort_values("risk_score", ascending=False).head(20)
    st.dataframe(top_risk)

    # ---------------------------
    # 6ï¸âƒ£ Select individual student
    # ---------------------------
    st.subheader("Check Individual Student Risk")
    student_idx = st.number_input("Enter student row index:", min_value=0, max_value=len(df_new)-1, value=0)
    student = df_new.iloc[student_idx]
    st.write(student)

    # ---------------------------
    # 7ï¸âƒ£ Model Comparison Graph
    # ---------------------------
    st.subheader("Model Comparison (ROC AUC on test set)")

    # Load test set metrics (precomputed)
    # Replace these with your real test set metrics if available
    roc_scores = {
        "Logistic Regression": 0.85,
        "Decision Tree": 0.78,
        "Random Forest": 0.88,
        "XGBoost": 0.90
    }

    models_list = list(roc_scores.keys())
    scores_list = list(roc_scores.values())

    plt.figure(figsize=(8,4))
    sns.barplot(x=models_list, y=scores_list, palette="viridis")
    plt.ylim(0,1)
    plt.ylabel("ROC AUC")
    plt.title("Model Performance Comparison")
    for i, v in enumerate(scores_list):
        plt.text(i, v+0.01, f"{v:.2f}", ha='center', fontweight='bold')
    st.pyplot(plt)
    !pip freeze > requirements.txt
